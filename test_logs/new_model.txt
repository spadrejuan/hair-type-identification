1st run: New Model
Epoch 50/50
25/25 ━━━━━━━━━━━━━━━━━━━━ 2s 74ms/step - accuracy: 0.8020 - loss: 0.5009 - val_accuracy: 0.6173 - val_loss: 1.0344

2nd run: Added Conv2D
Epoch 50/50
25/25 ━━━━━━━━━━━━━━━━━━━━ 3s 96ms/step - accuracy: 0.7240 - loss: 0.6413 - val_accuracy: 0.5816 - val_loss: 1.0233

3rd run: Added BatchNormalization and Added Global2DPooling. Added Dropout = 0.25
Epoch 50/50
25/25 ━━━━━━━━━━━━━━━━━━━━ 3s 107ms/step - accuracy: 0.7989 - loss: 0.4656 - val_accuracy: 0.5561 - val_loss: 1.4524

4th run: Try 256x256. Added 3rd run metrics to each Conv2D
Epoch 50/50
25/25 ━━━━━━━━━━━━━━━━━━━━ 12s 457ms/step - accuracy: 0.8307 - loss: 0.4282 - val_accuracy: 0.5765 - val_loss: 1.3643

5th run: Go back to 128x128. Add Learning Rate Scheduler
Epoch 50/50
25/25 ━━━━━━━━━━━━━━━━━━━━ 4s 129ms/step - accuracy: 0.6653 - loss: 0.7373 - val_accuracy: 0.5510 - val_loss: 0.9630 - learning_rate: 1.0000e-09

6th run: Add RandomTranslation, and adjusted Learning Rate Scheduler
Epoch 50/50
25/25 ━━━━━━━━━━━━━━━━━━━━ 3s 113ms/step - accuracy: 0.7110 - loss: 0.6580 - val_accuracy: 0.4949 - val_loss: 1.1324 - learning_rate: 1.2500e-04

7th run: Double Stack Conv2D. Remove Data Augmentation
Epoch 50/50
25/25 ━━━━━━━━━━━━━━━━━━━━ 17s 661ms/step - accuracy: 0.8651 - loss: 0.3764 - val_accuracy: 0.6327 - val_loss: 0.9400 - learning_rate: 9.7656e-07

8th run: Epoch = 30, Try Data Augmentation again.
Epoch 30/30
25/25 ━━━━━━━━━━━━━━━━━━━━ 18s 684ms/step - accuracy: 0.7777 - loss: 0.5253 - val_accuracy: 0.5408 - val_loss: 0.9188 - learning_rate: 1.2500e-04

9th run: Simplify mode
TEST: 0.9921, VAL: 0.6122

10th run: Add Dropout = 0.25
Epoch 30/30
25/25 ━━━━━━━━━━━━━━━━━━━━ 2s 62ms/step - accuracy: 0.9836 - loss: 0.0548 - val_accuracy: 0.6276 - val_loss: 1.7356 - learning_rate: 3.9063e-06

11th run: Try Lasso Regularizer for Absolute Value
Epoch 30/30
25/25 ━━━━━━━━━━━━━━━━━━━━ 2s 68ms/step - accuracy: 0.9773 - loss: 0.3130 - val_accuracy: 0.6122 - val_loss: 2.2398 - learning_rate: 7.8125e-06

12th run: Ridge Regularizer
Epoch 30/30
25/25 ━━━━━━━━━━━━━━━━━━━━ 2s 61ms/step - accuracy: 0.9839 - loss: 0.1191 - val_accuracy: 0.5969 - val_loss: 1.8617 - learning_rate: 3.9063e-06

13th run: Add Global2DPooling and BatchNormalization back
Epoch 30/30
25/25 ━━━━━━━━━━━━━━━━━━━━ 3s 83ms/step - accuracy: 0.8128 - loss: 0.5611 - val_accuracy: 0.6531 - val_loss: 0.8588 - learning_rate: 2.5000e-04

14th run: Remove Regularizer
Epoch 30/30
25/25 ━━━━━━━━━━━━━━━━━━━━ 2s 76ms/step - accuracy: 0.7407 - loss: 0.6075 - val_accuracy: 0.5561 - val_loss: 0.8666 - learning_rate: 6.2500e-05

15th run: Go back to 8th run model but with 224x224 images and 64 batch size
Epoch 30/30
13/13 ━━━━━━━━━━━━━━━━━━━━ 5s 313ms/step - accuracy: 0.9963 - loss: 0.0173 - val_accuracy: 0.5867 - val_loss: 1.7860 - learning_rate: 3.9063e-06

16th run: Go back to 128x128 and 32 batch. Add Ridge Regularizer to Dense layer
Epoch 30/30
25/25 ━━━━━━━━━━━━━━━━━━━━ 2s 64ms/step - accuracy: 0.9935 - loss: 0.1188 - val_accuracy: 0.6122 - val_loss: 1.8370 - learning_rate: 7.8125e-06

17th run: Lasso Regularizer
Epoch 30/30
25/25 ━━━━━━━━━━━━━━━━━━━━ 2s 63ms/step - accuracy: 0.8886 - loss: 0.5410 - val_accuracy: 0.6071 - val_loss: 1.2690 - learning_rate: 1.2500e-04

18th run: L1_L2 w/ default value
Epoch 30/30
25/25 ━━━━━━━━━━━━━━━━━━━━ 2s 66ms/step - accuracy: 0.9928 - loss: 0.0252 - val_accuracy: 0.6122 - val_loss: 2.0266 - learning_rate: 1.9531e-06

19th run: Try Nadam Optimizer
Epoch 30/30
25/25 ━━━━━━━━━━━━━━━━━━━━ 2s 69ms/step - accuracy: 0.9937 - loss: 0.0139 - val_accuracy: 0.6122 - val_loss: 1.9825 - learning_rate: 3.9063e-06

20th run: Try 730 Seed to prove whether the problem aligns with the dataset
Epoch 30/30
25/25 ━━━━━━━━━━━━━━━━━━━━ 2s 67ms/step - accuracy: 0.9940 - loss: 0.0109 - val_accuracy: 0.6990 - val_loss: 1.5212 - learning_rate: 3.9063e-06

21st run: Try Adamax
Epoch 30/30
25/25 ━━━━━━━━━━━━━━━━━━━━ 2s 60ms/step - accuracy: 0.9885 - loss: 0.0862 - val_accuracy: 0.7092 - val_loss: 1.0771 - learning_rate: 7.8125e-06

22nd run: Try with Strides = 1
Epoch 30/30
25/25 ━━━━━━━━━━━━━━━━━━━━ 2s 57ms/step - accuracy: 0.9838 - loss: 0.1162 - val_accuracy: 0.6837 - val_loss: 1.0499 - learning_rate: 7.8125e-06

23rd run: Try even simpler model, back to 1337 for uniformity with Demo
Epoch 30/30
25/25 ━━━━━━━━━━━━━━━━━━━━ 2s 57ms/step - accuracy: 0.9655 - loss: 0.2393 - val_accuracy: 0.6071 - val_loss: 0.9328 - learning_rate: 7.8125e-06

24th run: Add BatchNormalization and Dropout to 0.50. Add back the commented kernel in 23rd run:
Epoch 30/30
25/25 ━━━━━━━━━━━━━━━━━━━━ 4s 140ms/step - accuracy: 0.9366 - loss: 0.1957 - val_accuracy: 0.5306 - val_loss: 3.6960 - learning_rate: 1.9531e-06

25h run: Removed overfitting methods but try Data Augmentation again
Epoch 30/30
25/25 ━━━━━━━━━━━━━━━━━━━━ 3s 120ms/step - accuracy: 0.7850 - loss: 0.5328 - val_accuracy: 0.5918 - val_loss: 0.9244 - learning_rate: 1.5625e-05

26th run: 0.0001 LR with Adam
Epoch 30/30
25/25 ━━━━━━━━━━━━━━━━━━━━ 2s 60ms/step - accuracy: 0.9355 - loss: 0.2570 - val_accuracy: 0.6122 - val_loss: 1.0137 - learning_rate: 7.8125e-07

27th run: 0.001 LR with Adam
Epoch 30/30
25/25 ━━━━━━━━━━━━━━━━━━━━ 2s 61ms/step - accuracy: 0.9930 - loss: 0.0225 - val_accuracy: 0.6173 - val_loss: 2.0149 - learning_rate: 3.9063e-06

28th run: Try BatchSize to 16
Epoch 30/30
50/50 ━━━━━━━━━━━━━━━━━━━━ 2s 44ms/step - accuracy: 0.9963 - loss: 0.0079 - val_accuracy: 0.6429 - val_loss: 2.4778 - learning_rate: 3.9063e-06

29th run: Last try to add overfitting fixes like Dropout and BatchNormalization
Epoch 30/30
50/50 ━━━━━━━━━━━━━━━━━━━━ 4s 69ms/step - accuracy: 0.8173 - loss: 0.4732 - val_accuracy: 0.6429 - val_loss: 0.7207 - learning_rate: 6.2500e-05

30th run: Separate Activation and add DilationRate
Epoch 30/30
50/50 ━━━━━━━━━━━━━━━━━━━━ 4s 69ms/step - accuracy: 0.7105 - loss: 0.6865 - val_accuracy: 0.5969 - val_loss: 0.8259 - learning_rate: 3.1250e-05

31st run: Back to Base Model but add DilationRate(2,2)
Epoch 30/30
50/50 ━━━━━━━━━━━━━━━━━━━━ 2s 42ms/step - accuracy: 0.9961 - loss: 0.0151 - val_accuracy: 0.6429 - val_loss: 2.2510 - learning_rate: 3.9063e-06

32nd run: Removed DilationRate, add 256x256 with BatchSize=16
Epoch 30/30
50/50 ━━━━━━━━━━━━━━━━━━━━ 8s 164ms/step - accuracy: 0.9963 - loss: 0.0071 - val_accuracy: 0.6224 - val_loss: 2.5908 - learning_rate: 1.9531e-06

