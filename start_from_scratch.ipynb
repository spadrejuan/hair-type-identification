{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Source Code for Lab Exercise 3.\n",
    "## Steps\n",
    "1. Preprocess the Data.\n",
    "2. Create a Train and Test Split\n",
    "3. Create a Convolutional Neural Network using Keras.\n",
    "4. Experiment on Various Elements\n",
    "5. Discussion and Analysis of Experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Possible Things to Experiment on\n",
    "1. Other Preprocessing methods for Images.\n",
    "2. Adding Max Pooling.\n",
    "3. Changing number of Filters.\n",
    "4. Changing Kernel Size.\n",
    "5. Changing Learning Rate.\n",
    "6. Changing Optimizers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking for Invalid datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import imghdr\n",
    "import os\n",
    "\n",
    "DATA_DIR = \"hair_types\"\n",
    "IMAGE_EXTENSIONS = [\".png\", \".jpg\"]  # add there all your images file extensions\n",
    "\n",
    "img_type_accepted_by_tf = [\"bmp\", \"gif\", \"jpeg\", \"png\"]\n",
    "for filepath in Path(DATA_DIR).rglob(\"*\"):\n",
    "    if filepath.suffix.lower() in IMAGE_EXTENSIONS:\n",
    "        img_type = imghdr.what(filepath)\n",
    "        if img_type is None:\n",
    "            print(f\"{filepath} is not an image\")\n",
    "            os.remove(filepath)\n",
    "        elif img_type not in img_type_accepted_by_tf:\n",
    "            print(f\"{filepath} is a {img_type}, not accepted by TensorFlow\")\n",
    "            os.remove(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Images Section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Tensorflow and Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras # we define like this for IntelliSense\n",
    "\n",
    "# We define the constants for our images\n",
    "BATCH_SIZE = 32\n",
    "HEIGHT = 128\n",
    "WIDTH = 128\n",
    "IMAGE_SIZE = (HEIGHT, WIDTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Datasets for the Validation and Training splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = keras.preprocessing.image_dataset_from_directory(\n",
    "    directory=\"hair_types\",\n",
    "    batch_size=BATCH_SIZE,\n",
    "    image_size=IMAGE_SIZE,\n",
    "    seed=1337, # same as Demo\n",
    "    validation_split=0.2, # use 20% as validation\n",
    "    subset=\"training\",\n",
    "    label_mode=\"categorical\"\n",
    ")\n",
    "val_dataset = keras.preprocessing.image_dataset_from_directory(\n",
    "    directory=\"hair_types\",\n",
    "    batch_size=BATCH_SIZE,\n",
    "    image_size=IMAGE_SIZE,\n",
    "    seed=1337, # same as Demo\n",
    "    validation_split=0.2, # use 20% as validation\n",
    "    subset=\"validation\",\n",
    "    label_mode=\"categorical\"\n",
    ")\n",
    "# The target labels\n",
    "class_names = train_dataset.class_names\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the initial dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "for images, labels in train_dataset.take(1):\n",
    "  for i in range(9):\n",
    "    ax = plt.subplot(3, 3, i + 1)\n",
    "    plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "    plt.title(class_names[np.argmax(labels[i])])\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ways to Preprocess Images - Data Augmentation using keras.Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers # We import like this for IntelliSense\n",
    "data_augmentation_methods = keras.Sequential([\n",
    "   layers.RandomFlip('horizontal_and_vertical'),\n",
    "   layers.RandomRotation(factor=0.2),\n",
    "   layers.RandomBrightness(factor=0.2),\n",
    "   layers.RandomTranslation(height_factor=0.2, width_factor=0.2),  # Shift within 20% of image size\n",
    "   layers.RandomZoom(height_factor=0.2, width_factor=0.2),\n",
    "   layers.RandomContrast(factor=0.2),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing how these methods affect images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "for images, _ in train_dataset.take(1):\n",
    "    for i in range(9):\n",
    "        augmented_images = data_augmentation_methods(images)\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(np.array(augmented_images[0]).astype(\"uint8\"))\n",
    "        plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import data as tf_data\n",
    "# Configuring dataset for performance\n",
    "# Prefetching samples in GPU memory helps maximize GPU utilization.\n",
    "train_dataset = train_dataset.prefetch(tf_data.AUTOTUNE)\n",
    "val_dataset = val_dataset.prefetch(tf_data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Convolutional Neural Network using Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Adding Max Pooling.\n",
    "# 3. Changing number of Filters.\n",
    "# 4. Changing Kernel Size.\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "\n",
    "# Preprocessing Layer\n",
    "\n",
    "# Convolutional Layer\n",
    "\n",
    "# Output Layer\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Util Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Changing Learning Rate.\n",
    "# 6. Changing Optimizers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting a specific Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = \"hair_types/Curly_Hair/02dac897d1dec9ba8c057a11d041ada8--layered-natural-hair-natural-black-hairstyles.jpg\"\n",
    "img = tf.keras.preprocessing.image.load_img(img_path, target_size=(HEIGHT, WIDTH))\n",
    "img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, 0)  # Create batch axis\n",
    "\n",
    "# Predict probabilities\n",
    "predictions = model.predict(img_array)[0]  \n",
    "\n",
    "# Scale predictions to percentages\n",
    "total_percentage = sum(predictions)\n",
    "scaled_predictions = [p * 100 / total_percentage for p in predictions]\n",
    "\n",
    "class_names = [\"Curly Hair\", \"Straight Hair\", \"Wavy Hair\"]  \n",
    "percentage_str = \", \".join([\"%.2f percent %s\" % (p, class_names[i]) for i, p in enumerate(scaled_predictions)])\n",
    "print(\"This image is \" + percentage_str + \".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting a Random Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "folder_number = random.randint(1, 3)  # Generate a random number between 1-3\n",
    "if folder_number == 1:\n",
    "    dataset_dir = \"hair_types/Curly_Hair/\"\n",
    "elif folder_number == 2:\n",
    "    dataset_dir = \"hair_types/Straight_Hair/\"\n",
    "else:\n",
    "    dataset_dir = \"hair_types/Wavy_Hair/\"\n",
    "\n",
    "print(dataset_dir)\n",
    "file_list = os.listdir(dataset_dir)\n",
    "\n",
    "random_file = random.choice(file_list)\n",
    "\n",
    "image_path = os.path.join(dataset_dir, random_file)\n",
    "\n",
    "img = tf.keras.preprocessing.image.load_img(image_path, target_size=(HEIGHT, WIDTH))\n",
    "img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, 0)  # Create batch axis\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(img_array)[0]\n",
    "\n",
    "\n",
    "# Scale predictions to percentages\n",
    "total_percentage = sum(predictions)\n",
    "scaled_predictions = [p * 100 / total_percentage for p in predictions]\n",
    "\n",
    "class_names = [\"Curly Hair\", \"Straight Hair\", \"Wavy Hair\"]  \n",
    "percentage_str = \", \".join([\"%.2f percent %s\" % (p, class_names[i]) for i, p in enumerate(scaled_predictions)])\n",
    "print(\"This image is \" + percentage_str + \".\")\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization in Respect to Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = cnn.history['accuracy']\n",
    "val_acc = cnn.history['val_accuracy']\n",
    "\n",
    "loss = cnn.history['loss']\n",
    "val_loss = cnn.history['val_loss']\n",
    "\n",
    "epochs_range = range(EPOCHS)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
